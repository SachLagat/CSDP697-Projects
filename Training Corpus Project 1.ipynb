{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2dc216c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lagat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download ('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3aa970f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training corpus\n",
    "corpus = [\n",
    "    \"the dog saw a cat\",\n",
    "    \"the dog chased the cat\",\n",
    "    \"the cat climbed a tree\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19dfcbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords using NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "corpus = [' '.join([word for word in sentence.split() if word.lower() not in stop_words]) for sentence in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e742705",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a vocabulary and a word-to-index dictionary\n",
    "vocabulary = set(\" \".join(corpus).split())\n",
    "vocab_size = len(vocabulary)\n",
    "w_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4a7a755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the one-hot encoding using dictionary\n",
    "def one_hot(word, w_to_index):\n",
    "    one_hot_vector = np.zeros(len(w_to_index))\n",
    "    one_hot_vector[w_to_index[word]] = 1\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b17d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize neural network parameters\n",
    "input_size = len(w_to_index)\n",
    "output_size = len(w_to_index)\n",
    "hidden_size = 3\n",
    "\n",
    "np.random.seed(0)\n",
    "weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
    "biases_hidden = np.zeros(hidden_size)\n",
    "biases_output = np.zeros(output_size)\n",
    "weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
    "hidden_output = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb4b845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass function to calculate probabilities using softmax\n",
    "def forward_pass(input_vector):\n",
    "    global hidden_output\n",
    "    hidden_input = np.dot(input_vector, weights_input_hidden) + biases_hidden\n",
    "    hidden_output = 1 / (1 + np.exp(-hidden_input))\n",
    "    output_input = np.dot(hidden_output, weights_hidden_output) + biases_output\n",
    "    output_probabilities = softmax(output_input)\n",
    "    return output_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b92df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax function\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / exp_x.sum(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed599922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (backpropagation)\n",
    "learning_rate = 0.1\n",
    "epochs = 1500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76836904",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        for i, target_word in enumerate(words[:-1]):\n",
    "            input_word = words[i]\n",
    "            one_hot_target_word = one_hot(words[i + 1], w_to_index)\n",
    "\n",
    "            # Forward pass\n",
    "            input_vector = one_hot(input_word, w_to_index)\n",
    "            output_prob = forward_pass(input_vector)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = -np.log(output_prob.dot(one_hot_target_word))\n",
    "\n",
    "            # Backpropagation\n",
    "            delta_output = output_prob - one_hot_target_word\n",
    "            delta_hidden = np.dot(delta_output, weights_hidden_output.T) * hidden_output * (1 - hidden_output)\n",
    "\n",
    "            # Update weights and biases\n",
    "            weights_hidden_output -= learning_rate * np.outer(hidden_output, delta_output)\n",
    "            biases_output -= learning_rate * delta_output\n",
    "            weights_input_hidden -= learning_rate * np.outer(input_vector, delta_hidden)\n",
    "            biases_hidden -= learning_rate * delta_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9654c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test the network by generating probabilities for all words in the vocabulary\n",
    "target_word = 'cat'\n",
    "input_vector_cat = one_hot(target_word, w_to_index)\n",
    "output_prob = forward_pass(input_vector_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fba8a2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities for words following 'cat'\n",
      "cat: 0.0050\n",
      "dog: 0.0002\n",
      "tree: 0.0000\n",
      "chased: 0.0020\n",
      "saw: 0.0018\n",
      "climbed: 0.9910\n"
     ]
    }
   ],
   "source": [
    "# Print the probabilities for all words in the vocabulary\n",
    "print(\"Probabilities for words following 'cat'\")\n",
    "for word, prob in zip(w_to_index.keys(), output_prob):\n",
    "    print(f\"{word}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db8ed44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
